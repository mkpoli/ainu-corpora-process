{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Generate combined part of speech data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import json\n",
        "from typing import TypedDict\n",
        "\n",
        "OUTPUT_DIR = Path(\"output\")\n",
        "INPUT_DIR = Path(\"input\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load Wiktionary POS data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wiktionary entries: 2229\n",
            "wan\t['num']\n",
            "tu\t['num']\n",
            "rak\t['verb']\n",
            "ci\t['verb']\n",
            "mi\t['verb']\n",
            "on\t['verb']\n",
            "ona\t['noun']\n",
            "o\t['verb']\n",
            "ay\t['noun']\n",
            "oro\t['noun']\n",
            "he\t['parti']\n",
            "i\t['prefix']\n"
          ]
        }
      ],
      "source": [
        "# Load Wiktionary POS data\n",
        "with open(OUTPUT_DIR / \"wiktionary_ainu_part_of_speech.json\", \"r\") as f:\n",
        "    wiktionary_pos = json.load(f)\n",
        "\n",
        "print(\"Wiktionary entries:\", len(wiktionary_pos))\n",
        "for i, (lemma, pos) in enumerate(wiktionary_pos.items()):\n",
        "    print(f\"{lemma}\\t{pos}\")\n",
        "    if i > 10:\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load FF Ainu POS data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FF Saru entries: 975\n",
            "ア\t人接\n",
            "ア\t自\n",
            "アアン\t助動\n",
            "アイヌ\t名\n",
            "アイヌイタク\t名\n",
            "アイヌフラ\t名\n",
            "アイネ\t接助\n",
            "アエプ\t名\n",
            "アオイペプ\t名\n",
            "アオカ\t代名\n",
            "アキヒ\t名\n",
            "アク\t名\n"
          ]
        }
      ],
      "source": [
        "# Load FF Ainu POS data\n",
        "with open(OUTPUT_DIR / \"ff-ainu-saru-terms.json\", \"r\") as f:\n",
        "    ff_saru_terms = json.load(f)\n",
        "\n",
        "print(\"FF Saru entries:\", len(ff_saru_terms))\n",
        "for i, entry in enumerate(ff_saru_terms):\n",
        "    print(f\"{entry['kana']}\\t{entry['pos']}\")\n",
        "    if i > 10:\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load manual POS data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Manual: 2\n",
            "k=\t['pers']\n",
            "c=\t['pers']\n"
          ]
        }
      ],
      "source": [
        "with open(INPUT_DIR / \"manual_gloss.json\", \"r\") as f:\n",
        "    manual_glosses = {k: v[\"poses\"] for k, v in json.load(f).items()}\n",
        "\n",
        "print(\"Manual:\", len(manual_glosses))\n",
        "for i, (lemma, pos) in enumerate(manual_glosses.items()):\n",
        "    print(f\"{lemma}\\t{pos}\")\n",
        "    if i > 10:\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Combine POS data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Combined entries: 3186\n",
            "wan\t['num']\n",
            "tu\t['num']\n",
            "rak\t['verb']\n",
            "ci\t['verb']\n",
            "mi\t['verb']\n",
            "on\t['verb']\n",
            "ona\t['noun']\n",
            "o\t['verb']\n",
            "ay\t['noun']\n",
            "oro\t['noun']\n",
            "he\t['parti']\n",
            "i\t['prefix']\n"
          ]
        }
      ],
      "source": [
        "# Map FF Ainu POS tags to Wiktionary format\n",
        "POS_MAP = {\n",
        "    \"名\": \"noun\",\n",
        "    \"自\": \"verb\", \n",
        "    \"他\": \"verb\",\n",
        "    \"複他\": \"verb\",\n",
        "    \"完\": \"verb\",\n",
        "    \"助動\": \"auxverb\",\n",
        "    \"副\": \"adv\",\n",
        "    \"後副\": \"postpadv\",\n",
        "    \"連体\": \"adnom\",\n",
        "    \"間投\": \"interj\",\n",
        "    \"形名\": \"nmlz\",\n",
        "    \"代名\": \"pronoun\",\n",
        "    \"位名\": \"noun\",\n",
        "    \"格助\": \"parti\",\n",
        "    \"副助\": \"parti\",\n",
        "    \"終助\": \"parti\",\n",
        "    \"接助\": \"parti\",\n",
        "    \"人接\": \"pers\",\n",
        "    \"接頭\": \"prefix\",\n",
        "    \"接尾\": \"suffix\",\n",
        "    \"位名＋格助\": \"colloc\",\n",
        "    \"名＋格助\": \"colloc\",\n",
        "    \"動\": \"verb\"\n",
        "}\n",
        "\n",
        "# Combine POS data\n",
        "combined_pos = {}\n",
        "\n",
        "# Add Wiktionary entries\n",
        "for lemma, pos_list in wiktionary_pos.items():\n",
        "    combined_pos[lemma] = pos_list\n",
        "\n",
        "# Add FF Saru entries\n",
        "for entry in ff_saru_terms:\n",
        "    if entry[\"kana\"] in combined_pos and combined_pos[entry[\"kana\"]]:\n",
        "        continue\n",
        "\n",
        "    mapped_poses = []\n",
        "    for pos in entry[\"pos\"].split(\"／\"):\n",
        "        mapped_pos = POS_MAP.get(pos)\n",
        "        if mapped_pos:\n",
        "            mapped_poses.append(mapped_pos)\n",
        "        else:\n",
        "            print(f\"No mapping found for {entry['kana']} with POS {entry['pos']}\")\n",
        "\n",
        "    combined_pos[entry[\"kana\"]] = mapped_poses\n",
        "\n",
        "print(\"Combined entries:\", len(combined_pos))\n",
        "for i, (lemma, pos) in enumerate(combined_pos.items()):\n",
        "    print(f\"{lemma}\\t{pos}\")\n",
        "    if i > 10:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save combined POS data\n",
        "with open(OUTPUT_DIR / \"combined_part_of_speech.json\", \"w\") as f:\n",
        "    json.dump(combined_pos, f, ensure_ascii=False, indent=2)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
