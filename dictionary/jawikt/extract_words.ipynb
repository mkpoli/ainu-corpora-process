{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract vocabulary from Japanese Wiktionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "LATEST_WIKITIONARY_DUMP_URL = \"https://dumps.wikimedia.org/jawiktionary/latest/jawiktionary-latest-pages-articles-multistream.xml.bz2\"\n",
    "\n",
    "OUTPUT_DIR = Path(\"../output\")\n",
    "wiktionary_ainu_entries_json_path = OUTPUT_DIR / \"wiktionary_ainu_entries.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Ainu entries from Japanese Wiktionary dump data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d58e3bf4a1d64422844fe60ae7b6659a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/91.8M [00:00<?, ?iB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished downloading\n",
      "Finished decompressing\n",
      "Finished extracting\n"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "import wiktionary_dump_extractor\n",
    "import bz2\n",
    "import requests\n",
    "\n",
    "\n",
    "\n",
    "with tempfile.TemporaryDirectory() as temp_dir:\n",
    "    downloaded_path = Path(temp_dir) / \"jawiktionary-latest-pages-articles-multistream.xml.bz2\"\n",
    "\n",
    "    decompressed_path = Path(temp_dir) / \"jawiktionary-latest-pages-articles-multistream.xml\"\n",
    "\n",
    "    # Only download and process if dump is newer or output doesn't exist\n",
    "    with requests.get(LATEST_WIKITIONARY_DUMP_URL, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        total_size = int(r.headers.get('content-length', 0))\n",
    "        progress_bar = tqdm(total=total_size, unit='iB', unit_scale=True, desc=\"Downloading\")\n",
    "        with open(downloaded_path, \"wb\") as f:\n",
    "            for chunk in r.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "                progress_bar.update(len(chunk))\n",
    "        progress_bar.close()\n",
    "\n",
    "    print(\"Finished downloading\")\n",
    "\n",
    "    with open(decompressed_path, \"wb\") as f:\n",
    "        f.write(bz2.open(downloaded_path, \"rb\").read())\n",
    "\n",
    "    print(\"Finished decompressing\")\n",
    "\n",
    "    wiktionary.extract_ainu_entries(str(decompressed_path), str(wiktionary_ainu_entries_json_path))\n",
    "\n",
    "    print(\"Finished extracting\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Part of Speech information from extracted Ainu entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abc5dbb382d94398ae1f45e3563ddc2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2318 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ma\n",
      "hima\n",
      "tagis\n",
      "asu\n",
      "num\n",
      "pitu\n",
      "siva\n",
      "chwast\n",
      "----------------------------------------------------------------------------------------------------\n",
      "adnom\n",
      "adv\n",
      "auxverb\n",
      "colloc\n",
      "conj\n",
      "determiner\n",
      "interj\n",
      "noun\n",
      "num\n",
      "parti\n",
      "postpadv\n",
      "prefix\n",
      "pron\n",
      "pronoun\n",
      "rel\n",
      "root\n",
      "suffix\n",
      "verb\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import regex as re\n",
    "\n",
    "with open(wiktionary_ainu_entries_json_path, \"r\") as f:\n",
    "    wiktionary_ainu_entries = json.load(f)\n",
    "\n",
    "\n",
    "valid_entries = {}\n",
    "for entry in tqdm(wiktionary_ainu_entries):\n",
    "    if re.search(r\"^[a-z=\\-]+$\", entry[\"title\"]):\n",
    "        valid_entries[entry[\"title\"]] = entry[\"text\"]\n",
    "\n",
    "MAP = {\n",
    "    \"後置副詞\": \"postpadv\",\n",
    "    \"助詞\": \"parti\",\n",
    "    \"助動詞\": \"auxverb\",\n",
    "    \"位置名詞\": \"noun\",\n",
    "    \"動詞\": \"verb\",\n",
    "    \"副詞\": \"adv\",\n",
    "    \"疑問{{pronoun\": \"pronoun\",\n",
    "    \"関係詞\": \"rel\",\n",
    "    \"代名詞\": \"pron\",\n",
    "    \"接尾辞\": \"suffix\",\n",
    "    \"名詞\": \"noun\",\n",
    "    \"連体詞\": \"adnom\",\n",
    "    \"間投詞\": \"interj\",\n",
    "    \"数詞\": \"num\",\n",
    "    \"adverb\": \"adv\",\n",
    "    \"interjection\": \"interj\",\n",
    "    \"adjc\": \"verb\",\n",
    "    \"adjective\": \"verb\",\n",
    "    \"形容詞\": \"verb\",\n",
    "    \"conjunction\": \"conj\",\n",
    "    \"adnominal\": \"adnom\",\n",
    "    \"numeral\": \"num\",\n",
    "    \"pronoun\": \"pron\",\n",
    "    \"pref\": \"prefix\",\n",
    "    \"人称接辞\": None,\n",
    "    # non-pos\n",
    "    \"雨\": None,\n",
    "    \"鳥\": None,\n",
    "    \"魚\": None,\n",
    "    \"色\": None,\n",
    "    \"動物\": None,\n",
    "    \"擬音語\": None,\n",
    "    \"オノマトペ\": None,\n",
    "    \"果実\": None,\n",
    "    \"植物\": None,\n",
    "    \"食品\": None,\n",
    "    \"家族\": None,\n",
    "    \"神事\": None,\n",
    "}\n",
    "\n",
    "result = {}\n",
    "for title, text in valid_entries.items():\n",
    "    found = re.findall(r\"\\{\\{head\\|ain\\|(?:head=.*?\\|)?([^\\|\\}]+)[^\\}]*\\}\\}\", text)\n",
    "\n",
    "    if \"{{ain-verb\" in text or \"項動詞\" in text:\n",
    "        found.append(\"verb\")\n",
    "\n",
    "    if \"===成句===\" in text:\n",
    "        found.append(\"colloc\")\n",
    "\n",
    "    found += re.findall(\n",
    "        r\"\\[\\[(?:Category|カテゴリ):(?:\\{\\{ain\\}\\}|アイヌ語)[_ ]\\{?\\{?([^\\|\\}\\]]+)\\}?\\}?\",\n",
    "        text,\n",
    "    )\n",
    "\n",
    "    if title.startswith(\"-\") or title.startswith(\"=\"):\n",
    "        found.append(\"suffix\")\n",
    "    if title.endswith(\"-\") or title.endswith(\"=\"):\n",
    "        found.append(\"prefix\")\n",
    "\n",
    "    filtered = set(MAP[f] if f in MAP else f for f in found if f not in MAP or MAP[f])\n",
    "    result[title] = filtered\n",
    "\n",
    "result[\"tuki\"] = {\"noun\"}\n",
    "\n",
    "for title, pos in result.items():\n",
    "    if not pos:\n",
    "        print(title)\n",
    "    if any(not re.match(r\"^\\p{sc=Latn}+$\", p) for p in pos):\n",
    "        print(title, pos)\n",
    "\n",
    "with open(\n",
    "    OUTPUT_DIR / \"wiktionary_ainu_part_of_speech.json\", \"w\", encoding=\"utf-8\"\n",
    ") as f:\n",
    "    json.dump({k: list(v) for k, v in result.items()}, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"-\" * 100)\n",
    "all_pos = set()\n",
    "for v in result.values():\n",
    "    all_pos.update(v)\n",
    "\n",
    "for pos in sorted(all_pos):\n",
    "    print(pos)\n",
    "\n",
    "# TODO: UPOS - XPOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Etymology and Word Compositions from extracted Ainu entries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80a396a1463a4e3b8b33d9ca08c0a403",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2318 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "import regex as re\n",
    "\n",
    "with open(wiktionary_ainu_entries_json_path, \"r\") as f:\n",
    "    wiktionary_ainu_entries = json.load(f)\n",
    "\n",
    "from collections import defaultdict\n",
    "# map from morpheme to its etymology\n",
    "dictionary: dict[str, set[str]] = defaultdict(set)\n",
    "\n",
    "for entry in tqdm(iterable=wiktionary_ainu_entries):\n",
    "    if re.search(r\"\\{\\{affix\\|ain\\|.*?\\}\\}\", entry[\"text\"]):\n",
    "        for affix in re.finditer(r\"\\{\\{affix\\|ain\\|(.*)\\}\\}[。<]?\", entry[\"text\"]):\n",
    "            arguments = affix.group(1).replace(\"{{=}}\", \"=\").split(\"|\")\n",
    "            # print(arguments)\n",
    "\n",
    "            positional_arguments = []\n",
    "            keyword_arguments = {}\n",
    "\n",
    "            for argument in arguments:\n",
    "                if \"=\" in argument:\n",
    "                    key, value = argument.split(\"=\")\n",
    "                    keyword_arguments[key] = value\n",
    "                else:\n",
    "                    positional_arguments.append(argument)\n",
    "\n",
    "            # print(positional_arguments)\n",
    "            # print(keyword_arguments)\n",
    "\n",
    "            # TODO: handle {{m|ain|...}} + {{m|ain|...}}\n",
    "\n",
    "            for i, argument in enumerate(positional_arguments):\n",
    "                if f\"t{i+1}\" in keyword_arguments and keyword_arguments[f\"t{i+1}\"] != \"\":\n",
    "                    gloss = keyword_arguments[f\"t{i+1}\"].replace(\"～を\", \"\").split(\"}}\")[0]\n",
    "                    dictionary[argument].add(gloss)\n",
    "\n",
    "with open(OUTPUT_DIR / \"wiktionary_ainu_glossed_morphemes.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(\n",
    "        {k: list(v) for k, v in dictionary.items()}, f, ensure_ascii=False, indent=4\n",
    "    )\n",
    "\n",
    "    # print(entry[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77db78bca4434f52aed9316136e5bdf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2318 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# find ?\n",
    "for entry in tqdm(iterable=wiktionary_ainu_entries):\n",
    "    text = entry[\"text\"]\n",
    "    if \"{{etym}}\" in text or \"語源\" in text:\n",
    "        print(text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
