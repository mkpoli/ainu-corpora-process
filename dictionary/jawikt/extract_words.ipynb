{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract vocabulary from Japanese Wiktionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "LATEST_WIKITIONARY_DUMP_URL = \"https://dumps.wikimedia.org/jawiktionary/latest/jawiktionary-latest-pages-articles-multistream.xml.bz2\"\n",
    "\n",
    "OUTPUT_DIR = Path(\"../output\")\n",
    "wiktionary_ainu_entries_json_path = OUTPUT_DIR / \"wiktionary_ainu_entries.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Ainu entries from Japanese Wiktionary dump data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tempfile\n",
    "# import wiktionary_dump_extractor\n",
    "# import bz2\n",
    "# import requests\n",
    "\n",
    "\n",
    "# with tempfile.TemporaryDirectory() as temp_dir:\n",
    "#     downloaded_path = Path(temp_dir) / \"jawiktionary-latest-pages-articles-multistream.xml.bz2\"\n",
    "\n",
    "#     decompressed_path = Path(temp_dir) / \"jawiktionary-latest-pages-articles-multistream.xml\"\n",
    "\n",
    "#     # Only download and process if dump is newer or output doesn't exist\n",
    "#     with requests.get(LATEST_WIKITIONARY_DUMP_URL, stream=True) as r:\n",
    "#         r.raise_for_status()\n",
    "#         total_size = int(r.headers.get('content-length', 0))\n",
    "#         progress_bar = tqdm(total=total_size, unit='iB', unit_scale=True, desc=\"Downloading\")\n",
    "#         with open(downloaded_path, \"wb\") as f:\n",
    "#             for chunk in r.iter_content(chunk_size=8192):\n",
    "#                 f.write(chunk)\n",
    "#                 progress_bar.update(len(chunk))\n",
    "#         progress_bar.close()\n",
    "\n",
    "#     print(\"Finished downloading\")\n",
    "\n",
    "#     with open(decompressed_path, \"wb\") as f:\n",
    "#         f.write(bz2.open(downloaded_path, \"rb\").read())\n",
    "\n",
    "#     print(\"Finished decompressing\")\n",
    "\n",
    "#     wiktionary_dump_extractor.extract_ainu_entries(\n",
    "#         str(decompressed_path), str(wiktionary_ainu_entries_json_path)\n",
    "#     )\n",
    "\n",
    "#     print(\"Finished extracting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict\n",
    "\n",
    "class Entry(TypedDict):\n",
    "    title: str\n",
    "    text: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter out non-Ainu sections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import regex as re\n",
    "\n",
    "wiktionary_ainu_entries_filtered_json_path = OUTPUT_DIR / \"wiktionary_ainu_entries_filtered.json\"\n",
    "\n",
    "with open(wiktionary_ainu_entries_json_path, \"r\") as f:\n",
    "    wiktionary_ainu_entries: list[Entry] = json.load(f)\n",
    "\n",
    "filtered_entries = [\n",
    "    entry for entry in wiktionary_ainu_entries\n",
    "    if re.match(r\"^[\\p{sc=Latn}=\\- '’]+$\", entry[\"title\"])\n",
    "]\n",
    "# wiktionary_ainu_entries = [\n",
    "#     entry for entry in wiktionary_ainu_entries if entry[\"title\"]\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wiktionary.document import Document\n",
    "\n",
    "filtered_ainu_entries: dict[str, str] = {}\n",
    "for entry in wiktionary_ainu_entries:\n",
    "    doc = Document.from_wikitext(entry[\"text\"])\n",
    "    for section in doc.sections:\n",
    "        if section.title not in [\n",
    "            \"{{ain}}\",\n",
    "            \"{{L|ain}}\",\n",
    "            \"アイヌ語\",\n",
    "            \"[[:Category:{{ain}}|{{ain}}]]\",\n",
    "            \"[[:Category:{{ain}}|{{ain}}]]== <!--これは標準の内容の展開です。書き換えないでください-->\",\n",
    "        ]:\n",
    "            continue\n",
    "        filtered_ainu_entries[entry[\"title\"]] = str(object=section)\n",
    "        break\n",
    "    # except Exception as e:\n",
    "    #     print(e)\n",
    "    #     print(entry[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(wiktionary_ainu_entries_filtered_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(filtered_ainu_entries, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Part of Speech information from extracted Ainu entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ba3127acd284336bc647a48c5100d81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orun\n",
      "----------------------------------------------------------------------------------------------------\n",
      "adnom\n",
      "adv\n",
      "auxverb\n",
      "colloc\n",
      "conj\n",
      "determiner\n",
      "interj\n",
      "noun\n",
      "num\n",
      "parti\n",
      "postpadv\n",
      "prefix\n",
      "pron\n",
      "pronoun\n",
      "rel\n",
      "root\n",
      "suffix\n",
      "verb\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import regex as re\n",
    "\n",
    "with open(wiktionary_ainu_entries_filtered_json_path, \"r\") as f:\n",
    "    wiktionary_ainu_entries: dict[str, str] = json.load(f)\n",
    "\n",
    "\n",
    "valid_entries = {}\n",
    "for title, text in tqdm(list(wiktionary_ainu_entries.items())):\n",
    "    if re.search(r\"^[a-z=\\-]+$\", title):\n",
    "        valid_entries[title] = text\n",
    "\n",
    "MAP = {\n",
    "    \"後置副詞\": \"postpadv\",\n",
    "    \"助詞\": \"parti\",\n",
    "    \"助動詞\": \"auxverb\",\n",
    "    \"位置名詞\": \"noun\",\n",
    "    \"動詞\": \"verb\",\n",
    "    \"副詞\": \"adv\",\n",
    "    \"疑問{{pronoun\": \"pronoun\",\n",
    "    \"関係詞\": \"rel\",\n",
    "    \"代名詞\": \"pron\",\n",
    "    \"接尾辞\": \"suffix\",\n",
    "    \"名詞\": \"noun\",\n",
    "    \"連体詞\": \"adnom\",\n",
    "    \"間投詞\": \"interj\",\n",
    "    \"数詞\": \"num\",\n",
    "    \"adverb\": \"adv\",\n",
    "    \"interjection\": \"interj\",\n",
    "    \"adjc\": \"verb\",\n",
    "    \"adjective\": \"verb\",\n",
    "    \"形容詞\": \"verb\",\n",
    "    \"conjunction\": \"conj\",\n",
    "    \"adnominal\": \"adnom\",\n",
    "    \"numeral\": \"num\",\n",
    "    \"pronoun\": \"pron\",\n",
    "    \"pref\": \"prefix\",\n",
    "    \"人称接辞\": None,\n",
    "    # non-pos\n",
    "    \"雨\": None,\n",
    "    \"鳥\": None,\n",
    "    \"魚\": None,\n",
    "    \"色\": None,\n",
    "    \"動物\": None,\n",
    "    \"擬音語\": None,\n",
    "    \"オノマトペ\": None,\n",
    "    \"果実\": None,\n",
    "    \"植物\": None,\n",
    "    \"食品\": None,\n",
    "    \"家族\": None,\n",
    "    \"神事\": None,\n",
    "}\n",
    "\n",
    "result = {}\n",
    "for title, text in valid_entries.items():\n",
    "    found = re.findall(r\"\\{\\{head\\|ain\\|(?:head=.*?\\|)?([^\\|\\}]+)[^\\}]*\\}\\}\", text)\n",
    "\n",
    "    if \"{{ain-verb\" in text or \"項動詞\" in text:\n",
    "        found.append(\"verb\")\n",
    "\n",
    "    if \"===成句===\" in text:\n",
    "        found.append(\"colloc\")\n",
    "\n",
    "    found += re.findall(\n",
    "        r\"\\[\\[(?:Category|カテゴリ):(?:\\{\\{ain\\}\\}|アイヌ語)[_ ]\\{?\\{?([^\\|\\}\\]]+)\\}?\\}?\",\n",
    "        text,\n",
    "    )\n",
    "\n",
    "    if title.startswith(\"-\") or title.startswith(\"=\"):\n",
    "        found.append(\"suffix\")\n",
    "    if title.endswith(\"-\") or title.endswith(\"=\"):\n",
    "        found.append(\"prefix\")\n",
    "\n",
    "    filtered = set(MAP[f] if f in MAP else f for f in found if f not in MAP or MAP[f])\n",
    "    result[title] = filtered\n",
    "\n",
    "result[\"tuki\"] = {\"noun\"}\n",
    "\n",
    "for title, pos in result.items():\n",
    "    if not pos:\n",
    "        print(title)\n",
    "    if any(not re.match(r\"^\\p{sc=Latn}+$\", p) for p in pos):\n",
    "        print(title, pos)\n",
    "\n",
    "with open(\n",
    "    OUTPUT_DIR / \"wiktionary_ainu_part_of_speech.json\", \"w\", encoding=\"utf-8\"\n",
    ") as f:\n",
    "    json.dump({k: list(v) for k, v in result.items()}, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"-\" * 100)\n",
    "all_pos = set()\n",
    "for v in result.values():\n",
    "    all_pos.update(v)\n",
    "\n",
    "for pos in sorted(all_pos):\n",
    "    print(pos)\n",
    "\n",
    "# TODO: UPOS - XPOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Etymology and Word Compositions from extracted Ainu entries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15888b9e9511468aaaddac07bcd54bf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "import regex as re\n",
    "\n",
    "with open(wiktionary_ainu_entries_filtered_json_path, \"r\") as f:\n",
    "    wiktionary_ainu_entries: dict[str, str] = json.load(f)\n",
    "\n",
    "from collections import defaultdict\n",
    "# map from morpheme to its etymology\n",
    "dictionary: dict[str, set[str]] = defaultdict(set)\n",
    "\n",
    "# map from term to its composition and glossing\n",
    "composition_dictionary: dict[str, list[tuple[str, str]]] = defaultdict(list)\n",
    "\n",
    "for title, text in tqdm(list(wiktionary_ainu_entries.items())):\n",
    "    if re.search(r\"\\{\\{affix\\|ain\\|.*?\\}\\}\", text):\n",
    "        for affix in re.finditer(r\"\\{\\{affix\\|ain\\|(.*)\\}\\}[。<]?\", text):\n",
    "            arguments = affix.group(1).replace(\"{{=}}\", \"=\").split(\"|\")\n",
    "            # print(arguments)\n",
    "\n",
    "            positional_arguments = []\n",
    "            keyword_arguments = {}\n",
    "\n",
    "            for argument in arguments:\n",
    "                if \"=\" in argument:\n",
    "                    key, value = argument.split(\"=\")\n",
    "                    keyword_arguments[key] = value\n",
    "                else:\n",
    "                    positional_arguments.append(argument)\n",
    "\n",
    "            # print(positional_arguments)\n",
    "            # print(keyword_arguments)\n",
    "\n",
    "            # TODO: handle {{m|ain|...}} + {{m|ain|...}}\n",
    "\n",
    "            for i, argument in enumerate(positional_arguments):\n",
    "                if f\"t{i+1}\" in keyword_arguments and keyword_arguments[f\"t{i+1}\"] != \"\":\n",
    "                    gloss = keyword_arguments[f\"t{i+1}\"].replace(\"～を\", \"\").split(\"}}\")[0]\n",
    "                    dictionary[argument].add(gloss)\n",
    "                    composition_dictionary[title].append((argument, gloss))\n",
    "with open(OUTPUT_DIR / \"wiktionary_ainu_glossed_morphemes.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(\n",
    "        {k: list(v) for k, v in dictionary.items()}, f, ensure_ascii=False, indent=4\n",
    "    )\n",
    "\n",
    "with open(OUTPUT_DIR / \"wiktionary_ainu_word_compositions.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(\n",
    "        {k: [tuple(t) for t in v] for k, v in composition_dictionary.items()}, f, ensure_ascii=False, indent=4\n",
    "    )\n",
    "\n",
    "    # print(entry[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Word Glosses\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5562c2658d054186a608cdd0eaf27fa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "import regex as re\n",
    "from typing import TypedDict\n",
    "\n",
    "with open(wiktionary_ainu_entries_filtered_json_path, \"r\") as f:\n",
    "    wiktionary_ainu_entries: dict[str, str] = json.load(f)\n",
    "\n",
    "from wiktionary.document import Document\n",
    "\n",
    "\n",
    "class Entry(TypedDict):\n",
    "    lemma: str\n",
    "    pos: str\n",
    "    glosses: list[str]\n",
    "\n",
    "\n",
    "gloss_dictionary: dict[str, Entry] = {}\n",
    "\n",
    "for title, text in tqdm(list(wiktionary_ainu_entries.items())):\n",
    "    # print(title)\n",
    "    if not re.match(r\"^[\\p{scx=Latn}=\\- '’0-9_'’]+$\", title):\n",
    "        continue\n",
    "\n",
    "    doc = Document.from_wikitext(text)\n",
    "    for section in doc.sections[0].subsections:\n",
    "        if section.title in [\n",
    "            \"{{pron}}\",\n",
    "            \"{{etym}}\",\n",
    "            \"{{etym}}1\",\n",
    "            \"{{etym}}2\",\n",
    "            \"参考文献\",\n",
    "            \"出典\",\n",
    "        ]:\n",
    "            continue\n",
    "        # print(\"  \" + section.title)\n",
    "        result_glosses = set()\n",
    "\n",
    "        for line in section.content.splitlines():\n",
    "\n",
    "            if line.startswith(\"#\"):\n",
    "                if (\n",
    "                    line.startswith(\"#*\")\n",
    "                    or line.startswith(\"#**\")\n",
    "                    or line.startswith(\"#:\")\n",
    "                    or line.startswith(\"##\")\n",
    "                ):\n",
    "                    continue\n",
    "                glosses = line.split(\"#\", 1)[1].strip()\n",
    "                glosses = re.sub(r\"（.*?）\", \"\", glosses)\n",
    "                glosses = re.sub(r\"\\(.*?\\)\", \"\", glosses)\n",
    "                glosses = re.sub(r\"<!--.*?-->\", \"\", glosses)\n",
    "                glosses = re.sub(r\"→.*$\", \"\", glosses)\n",
    "                for gloss in re.split(r\"[、。]\", glosses):\n",
    "                    gloss = gloss.strip()\n",
    "                    if not gloss:\n",
    "                        continue\n",
    "                    gloss = gloss.replace(\"〜\", \"～\")\n",
    "                    gloss = gloss.replace(\"……\", \"～\")\n",
    "                    gloss = gloss.replace(\"…\", \"～\")\n",
    "                    gloss = gloss.replace(\"...\", \"～\")\n",
    "                    gloss = re.sub(r\"\\[\\[.*?\\|(.*?)\\]\\]\", r\"\\1\", gloss)\n",
    "                    gloss = re.sub(r\"\\[\\[(.*?)\\]\\]\", r\"\\1\", gloss)\n",
    "                    gloss = re.sub(r\"［.*?］\", \"\", gloss)\n",
    "                    gloss = re.sub(r\"\\{\\{ふりがな\\|(.*?)\\|(.*?)\\}\\}\", r\"\\1\", gloss)\n",
    "                    gloss = re.sub(\n",
    "                        r\"\\{\\{おくりがな2\\|(.*?)\\|(.*?)\\|(.*?)\\|(.*?)\\}\\}\",\n",
    "                        r\"\\1\\3\",\n",
    "                        gloss,\n",
    "                    )\n",
    "                    gloss = re.sub(\n",
    "                        r\"\\{\\{おくりがな3\\|(.*?)\\|(.*?)\\|(.*?)\\|(.*?)\\|(.*?)\\|(.*?)\\|(.*?)\\}\\}\",\n",
    "                        r\"\\1\\3\\4\\6\",\n",
    "                        gloss,\n",
    "                    )\n",
    "                    gloss = re.sub(r\"\\{\\{(?:lb|context|タグ|l)\\|(.*?)\\}\\}\", r\"\", gloss)\n",
    "                    gloss = re.sub(r\"<ref.*\", r\"\", gloss)\n",
    "                    gloss = gloss.replace(\"'\", \"\")\n",
    "                    gloss = gloss.strip()\n",
    "                    if gloss.startswith(\"cf.\"):\n",
    "                        continue\n",
    "                    if \"noun form of\" in gloss:\n",
    "                        gloss = re.sub(\n",
    "                            r\"\\{\\{noun form of ?\\|ain\\|(.*?)\\|.*\\|所属形.*\",\n",
    "                            r\"\\1の所属形\",\n",
    "                            gloss,\n",
    "                        )\n",
    "                    if \"form of\" in gloss:\n",
    "                        gloss = re.sub(\n",
    "                            r\"\\{\\{form of ?\\|.*所属形\\|(?:tr=.*?\\|)?(.*?)(?:\\}|\\|)\",\n",
    "                            r\"\\1の所属形\",\n",
    "                            gloss,\n",
    "                        )\n",
    "                    if \"alternative form of\" in gloss:\n",
    "                        if \"t=\" in gloss:\n",
    "                            gloss = re.sub(\n",
    "                                r\"\\{\\{alternative form of\\|ain\\|(.*?)(?:\\|t=(.*?))?\\}\\}\",\n",
    "                                r\"\\2の別形\",\n",
    "                                gloss,\n",
    "                            )\n",
    "                        else:\n",
    "                            gloss = re.sub(\n",
    "                                r\"\\{\\{alternative form of\\|ain\\|(.*?)\\}\\}\",\n",
    "                                r\"\\1の別形\",\n",
    "                                gloss,\n",
    "                            )\n",
    "\n",
    "                    if \"verb form of\" in gloss:\n",
    "                        gloss = re.sub(\n",
    "                            r\"\\{\\{verb form of\\|ain\\|(.*?)\\|.*\\|(?:tr=.*?\\|)?t=(.*?)($|\\}\\})\",\n",
    "                            r\"\\2\",\n",
    "                            gloss,\n",
    "                        )\n",
    "                        # plural of|a|lang=ain\n",
    "                    if \"plural of\" in gloss:\n",
    "                        gloss = re.sub(\n",
    "                            r\"\\{\\{plural of\\|(.*?)\\|\",\n",
    "                            r\"\\1\",\n",
    "                            gloss,\n",
    "                        )\n",
    "\n",
    "                        # {'verb form of|ain|rewsian||p|tr={{ain-kana-conv|rewsian}}|t=一晩泊まる'}\n",
    "                    gloss = gloss.strip(\"{}\")\n",
    "\n",
    "                    if \"{{quote|ain\" in gloss:\n",
    "                        continue\n",
    "                    if \"|from=\" in gloss:\n",
    "                        continue\n",
    "                    if \"ux\" in gloss:\n",
    "                        continue\n",
    "                    if \"|ref=\" in gloss:\n",
    "                        continue\n",
    "                    if \"akana\" in gloss:\n",
    "                        continue\n",
    "                    if \"|t=\" in gloss:\n",
    "                        continue\n",
    "                    if \"lang|\" in gloss:\n",
    "                        continue\n",
    "                    if gloss:\n",
    "                        result_glosses.add(gloss)\n",
    "\n",
    "        # print(\"    \" + str(result_glosses))\n",
    "\n",
    "        cleaned_pos = section.title.strip(\"{}\")\n",
    "\n",
    "        pos = MAP[cleaned_pos] if cleaned_pos in MAP else cleaned_pos\n",
    "\n",
    "        gloss_dictionary[title] = {\n",
    "            \"lemma\": title,\n",
    "            \"pos\": pos,\n",
    "            \"glosses\": list(result_glosses),\n",
    "        }\n",
    "\n",
    "        # recleaned_glosses = []\n",
    "        # for gloss in cleaned_glosses:\n",
    "        #     if \"noun form of\" in gloss:\n",
    "        #         repl = re.sub(\n",
    "        #             r\"\\{\\{noun form of\\|ain\\|(.*?)\\|\\|所属形\\|(.*?)\\}\\}\\|(.*?\\}\\}\",\n",
    "        #             r\"\\1\",\n",
    "        #             gloss,\n",
    "        #         )\n",
    "        # recleaned_glosses.append(cleaned_gloss[repl])\n",
    "        #\n",
    "        # print(cleaned_glosses)\n",
    "# filtered_ainu_entries: dict[str, str] = {}\n",
    "# for entry in wiktionary_ainu_entries:\n",
    "#     doc = Document.from_wikitext(entry[\"text\"])\n",
    "#     for section in doc.sections:\n",
    "#         if section.title not in [\n",
    "#             \"{{ain}}\",\n",
    "#             \"{{L|ain}}\",\n",
    "#             \"アイヌ語\",\n",
    "#             \"[[:Category:{{ain}}|{{ain}}]]\",\n",
    "#             \"[[:Category:{{ain}}|{{ain}}]]== <!--これは標準の内容の展開です。書き換えないでください-->\",\n",
    "#         ]:\n",
    "#             continue\n",
    "#         filtered_ainu_entries[entry[\"title\"]] = str(object=section)\n",
    "#         break\n",
    "#     # except Exception as e:\n",
    "#     #     print(e)\n",
    "#     #     print(entry[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i tranverb}}を{{intrverb}}化する\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "dictionary changed size during iteration",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[416], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mentry\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgloss_dictionary\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgloss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mentry\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mglosses\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mの所属形\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgloss\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: dictionary changed size during iteration"
     ]
    }
   ],
   "source": [
    "for i, entry in enumerate(gloss_dictionary.values()):\n",
    "    for gloss in entry[\"glosses\"]:\n",
    "        if \"の所属形\" in gloss:\n",
    "            continue\n",
    "        if \"の別形\" in gloss:\n",
    "            continue\n",
    "        if \"参照\" in gloss:\n",
    "            continue\n",
    "        if \"を見よ\" in gloss:\n",
    "            continue\n",
    "        if re.match(r\"[^\\p{scx=Han}\\p{scx=Hira}\\p{scx=Kana}～]\", gloss):\n",
    "            print(entry[\"lemma\"], gloss)\n",
    "            del gloss_dictionary[entry[\"lemma\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lemma': 'wan', 'pos': 'num', 'glosses': ['十人', '十']}\n",
      "{'lemma': 'tu', 'pos': 'num', 'glosses': ['両方', '第二', '二つ', '多くの', '二人', '沢山の']}\n",
      "{'lemma': 'rak', 'pos': 'verb', 'glosses': ['～の気配がある', '～の匂いがする', '～の味がある']}\n",
      "{'lemma': 'mi', 'pos': 'verb', 'glosses': ['～を着る']}\n",
      "{'lemma': 'on', 'pos': 'verb', 'glosses': ['発酵する']}\n",
      "{'lemma': 'ona', 'pos': 'noun', 'glosses': ['父親']}\n",
      "{'lemma': 'ay', 'pos': 'noun', 'glosses': ['矢']}\n",
      "{'lemma': 'oro', 'pos': 'noun', 'glosses': ['強調する', '～の所', '場所をあらわす名詞の後に置いて', '～の場所として扱えない名詞の後に置いて場所を表す名詞句を作る', '所属形 oro ですでに言及した場所を示す']}\n",
      "{'lemma': 'he', 'pos': 'parti', 'glosses': ['～か']}\n",
      "{'lemma': 'i', 'pos': 'prefix', 'glosses': ['意味的に目的語を補い', 'それ', 'tranverb}}を{{intrverb}}化する']}\n",
      "{'lemma': 'ne', 'pos': 'parti', 'glosses': ['～に']}\n"
     ]
    }
   ],
   "source": [
    "for i, entry in enumerate(gloss_dictionary.values()):\n",
    "    if i > 10:\n",
    "        break\n",
    "    print(entry)\n",
    "\n",
    "with open(OUTPUT_DIR / \"wiktionary_ainu_glosses.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(gloss_dictionary, f, ensure_ascii=False, indent=4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
