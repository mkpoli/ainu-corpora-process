{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('output/ff-ainu-karahuto-terms.json') as f:\n",
    "    terms = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAKHALIN_BOOKS = {\n",
    "    \"からふとのアイヌご（入門）\",\n",
    "    \"カラフトのアイヌ語（中級）\",\n",
    "    \"カラフトのアイヌ語（初級）\",\n",
    "    \"ニューエクスプレス・スペシャル 日本語の隣人たち I+II\",\n",
    "    \"ピウスツキ記念碑\",\n",
    "    \"千徳太郎治のピウスツキ宛書簡\",\n",
    "    \"浅井タケ昔話全集I,II\",\n",
    "}\n",
    "\n",
    "words: set[tuple[str, int]] = set()\n",
    "\n",
    "for book in SAKHALIN_BOOKS:\n",
    "    with open(\"../corpus/output/words_by_book/\" + book + \".tsv\") as f:\n",
    "        for line in f:\n",
    "            word, freq = line.strip().split(\"\\t\")\n",
    "            words.add((word, int(freq)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adn\n",
      "adv\n",
      "advp\n",
      "aux\n",
      "auxv\n",
      "colloc\n",
      "conj\n",
      "cop\n",
      "int\n",
      "intj\n",
      "n\n",
      "nl\n",
      "nmlz\n",
      "num\n",
      "padv\n",
      "parti\n",
      "pers\n",
      "pfx\n",
      "pl\n",
      "postp\n",
      "pron\n",
      "rel\n",
      "root\n",
      "sfp\n",
      "sfx\n",
      "v\n",
      "vc\n",
      "vd\n",
      "vi\n",
      "vt\n"
     ]
    }
   ],
   "source": [
    "import regex\n",
    "from utils.sakhalin import extrapolate_sakhalin_from_hokkaido\n",
    "from typing import TypedDict\n",
    "\n",
    "\n",
    "terms_index = [term[\"lemma\"] for term in terms]\n",
    "\n",
    "class Word(TypedDict):\n",
    "    lemma: str\n",
    "    glosses: list[str]\n",
    "    poses: list[str]\n",
    "    notes: str\n",
    "    frequency: int\n",
    "\n",
    "with open(\"output/wiktionary_ainu_part_of_speech.json\") as f:\n",
    "    part_of_speech = json.load(f)\n",
    "    extrapolated_part_of_speech = {}\n",
    "    for lemma, poses in part_of_speech.items():\n",
    "        replaced = extrapolate_sakhalin_from_hokkaido(lemma)\n",
    "        if replaced not in part_of_speech:\n",
    "                extrapolated_part_of_speech[replaced] = poses\n",
    "\n",
    "combined_part_of_speech = {**part_of_speech, **extrapolated_part_of_speech}\n",
    "\n",
    "extrapolated_glosses = {}\n",
    "\n",
    "with open(\"output/wiktionary_ainu_glossed_morphemes.json\") as f:\n",
    "    glossed_morphemes = json.load(f)\n",
    "    for morpheme, glosses in glossed_morphemes.items():\n",
    "        replaced = extrapolate_sakhalin_from_hokkaido(morpheme)\n",
    "        if replaced not in glossed_morphemes:\n",
    "            extrapolated_glosses[replaced] = glosses\n",
    "\n",
    "combined_glosses = {**glossed_morphemes, **extrapolated_glosses}\n",
    "\n",
    "for (word, freq) in words:\n",
    "    if word not in terms_index:\n",
    "        terms.append({\"lemma\": word, \"glosses\": combined_glosses.get(word, []), \"poses\": combined_part_of_speech.get(word, []), \"frequency\": freq})\n",
    "    else:\n",
    "        term = next(term for term in terms if term[\"lemma\"] == word)\n",
    "        term[\"frequency\"] = freq\n",
    "\n",
    "for term in terms:\n",
    "    if term[\"lemma\"].endswith(\"hci\"):\n",
    "        if term[\"lemma\"].endswith(\"ahci\"):\n",
    "            singular = term[\"lemma\"][:-3]\n",
    "        else:\n",
    "            singular = term[\"lemma\"][:-2]\n",
    "        if singular in terms_index:\n",
    "            term[\"glosses\"] = next(t for t in terms if t[\"lemma\"] == singular)[\"glosses\"]\n",
    "            term[\"poses\"] = next(t for t in terms if t[\"lemma\"] == singular)[\"poses\"]\n",
    "\n",
    "\n",
    "POS_TABLE = {\n",
    "    \"自動詞\": \"vt\",\n",
    "    \"他動詞\": \"vi\",\n",
    "    \"複他動詞\": \"vd\",\n",
    "    \"完全動詞\": \"vc\",\n",
    "    \"名詞\": \"n\",\n",
    "    \"連体詞\": \"adn\",\n",
    "    \"形容詞\": \"adj\",\n",
    "    \"副詞\": \"adv\",\n",
    "    \"接続詞\": \"cconj\",\n",
    "    \"助詞\": \"post\",\n",
    "    \"助動詞\": \"auxv\",\n",
    "    \"auxverb\": \"auxv\",\n",
    "    \"終助詞\": \"sfp\",\n",
    "    \"接尾辞\": \"sfx\",\n",
    "    \"接頭辞\": \"pfx\",\n",
    "    \"間投詞\": \"intj\",\n",
    "    \"interj\": \"intj\",\n",
    "    \"後置副詞\": \"padv\",\n",
    "    \"人称接辞\": \"pers\",\n",
    "    \"繋辞\": \"cop\",\n",
    "    \"位置名詞\": \"nl\",\n",
    "    \"複数形\": \"pl\",\n",
    "    \"副助詞\": \"advp\",\n",
    "    \"suffix\": \"sfx\",\n",
    "    \"prefix\": \"pfx\",\n",
    "    \"verb\": \"v\", # TODO: Get verb slot from Wiktionary\n",
    "    \"代名詞\": \"pron\",\n",
    "    \"格助詞\": \"postp\",\n",
    "    \"接続助詞\": \"sconj\",\n",
    "    \"疑問詞\": \"int\",\n",
    "    \"形式名詞\": \"nmlz\",\n",
    "    \"noun\": \"n\"\n",
    "}\n",
    "\n",
    "\n",
    "for term in terms:\n",
    "    term[\"poses\"] = [POS_TABLE.get(p, None) or p for p in term[\"poses\"]]\n",
    "\n",
    "all_poses = set()\n",
    "for term in terms:\n",
    "    for p in term[\"poses\"]:\n",
    "        all_poses.add(p)\n",
    "for p in sorted(all_poses):\n",
    "    print(p)\n",
    "\n",
    "terms = [t for t in terms if t.get(\"frequency\", 0) > 1 or t[\"glosses\"]]\n",
    "\n",
    "terms = [t for t in terms if not regex.match(r\"^\\d+$\", t[\"lemma\"])]\n",
    "\n",
    "\n",
    "with open(\"output/ff-ainu-karahuto-terms-with-corpus.json\", \"w\") as f:\n",
    "    json.dump(terms, f, ensure_ascii=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
