{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('output/ff-ainu-karahuto-terms.json') as f:\n",
    "    terms = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('taa', 6697), ('manu', 2356), ('teh', 1320), ('ike', 1198), ('kusu', 1107), ('nah', 1054), ('tani', 1016), ('neampe', 919), ('horokewpo', 892), ('orowa', 854)]\n"
     ]
    }
   ],
   "source": [
    "SAKHALIN_BOOKS = {\n",
    "    \"からふとのアイヌご（入門）\",\n",
    "    \"カラフトのアイヌ語（中級）\",\n",
    "    \"カラフトのアイヌ語（初級）\",\n",
    "    \"ニューエクスプレス・スペシャル 日本語の隣人たち I+II\",\n",
    "    \"ピウスツキ記念碑\",\n",
    "    \"千徳太郎治のピウスツキ宛書簡\",\n",
    "    \"浅井タケ昔話全集I,II\",\n",
    "}\n",
    "\n",
    "words: set[tuple[str, int]] = set()\n",
    "\n",
    "for book in SAKHALIN_BOOKS:\n",
    "    with open(\"../corpus/output/words_by_book/\" + book + \".tsv\") as f:\n",
    "        for line in f:\n",
    "            word, freq = line.strip().split(\"\\t\")\n",
    "            words.add((word, int(freq)))\n",
    "\n",
    "print(sorted(words, key=lambda x: x[1], reverse=True)[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['５つの']\n",
      "['５つ']\n",
      "['歩く']\n",
      "['弟', '坊や']\n",
      "['弟', '坊や']\n"
     ]
    }
   ],
   "source": [
    "terms = [\n",
    "    {\n",
    "        \"lemma\": term[\"lemma\"],\n",
    "        \"ja\": term[\"glosses\"],\n",
    "        \"en\": [],\n",
    "        \"ru\": [],\n",
    "        \"poses\": term[\"poses\"],\n",
    "        \"frequency\": 0,\n",
    "    }\n",
    "    for term in terms\n",
    "]\n",
    "\n",
    "for t in terms[0:5]:\n",
    "    print(t[\"ja\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input/sakhalin/ff-ainu-karahuto-terms-with-corpus-translated-12.json\n",
      "input/sakhalin/ff-ainu-karahuto-terms-with-corpus-translated-2.json\n",
      "input/sakhalin/ff-ainu-karahuto-terms-with-corpus-translated-20.json\n",
      "input/sakhalin/ff-ainu-karahuto-terms-with-corpus-translated-8.json\n",
      "input/sakhalin/ff-ainu-karahuto-terms-with-corpus-translated-15.json\n",
      "input/sakhalin/ff-ainu-karahuto-terms-with-corpus-translated-4.json\n",
      "input/sakhalin/ff-ainu-karahuto-terms-with-corpus-translated-1.json\n",
      "input/sakhalin/ff-ainu-karahuto-terms-with-corpus-translated-13.json\n",
      "input/sakhalin/ff-ainu-karahuto-terms-with-corpus-translated-16.json\n",
      "input/sakhalin/ff-ainu-karahuto-terms-with-corpus-translated-10.json\n",
      "input/sakhalin/ff-ainu-karahuto-terms-with-corpus-translated-19.json\n",
      "input/sakhalin/ff-ainu-karahuto-terms-with-corpus-translated-7.json\n",
      "input/sakhalin/ff-ainu-karahuto-terms-with-corpus-translated-14.json\n",
      "input/sakhalin/ff-ainu-karahuto-terms-with-corpus-translated-17.json\n",
      "input/sakhalin/ff-ainu-karahuto-terms-with-corpus-translated-0.json\n",
      "input/sakhalin/ff-ainu-karahuto-terms-with-corpus-translated-9.json\n",
      "input/sakhalin/ff-ainu-karahuto-terms-with-corpus-translated-3.json\n",
      "input/sakhalin/ff-ainu-karahuto-terms-with-corpus-translated-11.json\n",
      "input/sakhalin/ff-ainu-karahuto-terms-with-corpus-translated-18.json\n",
      "input/sakhalin/ff-ainu-karahuto-terms-with-corpus-translated-5.json\n",
      "input/sakhalin/ff-ainu-karahuto-terms-with-corpus-translated-6.json\n",
      "adn\n",
      "adv\n",
      "advp\n",
      "auxv\n",
      "cconj\n",
      "colloc\n",
      "cop\n",
      "int\n",
      "intj\n",
      "n\n",
      "nl\n",
      "nmlz\n",
      "num\n",
      "padv\n",
      "parti\n",
      "pers\n",
      "pfx\n",
      "pl\n",
      "postp\n",
      "pron\n",
      "rel\n",
      "root\n",
      "sconj\n",
      "sfp\n",
      "sfx\n",
      "v\n",
      "vc\n",
      "vd\n",
      "vi\n",
      "vt\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import regex\n",
    "from utils.sakhalin import extrapolate_sakhalin_from_hokkaido\n",
    "from typing import TypedDict\n",
    "\n",
    "\n",
    "terms_index = [term[\"lemma\"] for term in terms]\n",
    "\n",
    "class Word(TypedDict):\n",
    "    lemma: str\n",
    "    glosses: list[str]\n",
    "    poses: list[str]\n",
    "    notes: str\n",
    "    frequency: int\n",
    "\n",
    "with open(\"output/wiktionary_ainu_part_of_speech.json\") as f:\n",
    "    part_of_speech = json.load(f)\n",
    "    extrapolated_part_of_speech = {}\n",
    "    for lemma, poses in part_of_speech.items():\n",
    "        replaced = extrapolate_sakhalin_from_hokkaido(lemma)\n",
    "        if replaced not in part_of_speech:\n",
    "                extrapolated_part_of_speech[replaced] = poses\n",
    "\n",
    "combined_part_of_speech = {**part_of_speech, **extrapolated_part_of_speech}\n",
    "\n",
    "extrapolated_glosses = {}\n",
    "\n",
    "with open(\"output/wiktionary_ainu_glossed_morphemes.json\") as f:\n",
    "    glossed_morphemes = json.load(f)\n",
    "    for morpheme, glosses in glossed_morphemes.items():\n",
    "        replaced = extrapolate_sakhalin_from_hokkaido(morpheme)\n",
    "        if replaced not in glossed_morphemes:\n",
    "            extrapolated_glosses[replaced] = glosses\n",
    "\n",
    "combined_glosses = {**glossed_morphemes, **extrapolated_glosses}\n",
    "\n",
    "for (word, freq) in words:\n",
    "    if word not in terms_index:\n",
    "        terms.append({\"lemma\": word, \"ja\": combined_glosses.get(word, []), \"poses\": combined_part_of_speech.get(word, []), \"frequency\": freq})\n",
    "    else:\n",
    "        term = next(term for term in terms if term[\"lemma\"] == word)\n",
    "        term[\"frequency\"] = freq\n",
    "\n",
    "for file in (Path(\"input\") / \"sakhalin\").glob(\n",
    "    \"ff-ainu-karahuto-terms-with-corpus-translated-*.json\"\n",
    "):\n",
    "    print(file)\n",
    "    with open(file) as f:\n",
    "        translated = json.load(f)\n",
    "\n",
    "        for term in terms:\n",
    "            if term[\"lemma\"] in translated:\n",
    "                term[\"ja\"] = translated[term[\"lemma\"]][\"ja\"]\n",
    "                term[\"en\"] = translated[term[\"lemma\"]][\"en\"]\n",
    "                term[\"ru\"] = translated[term[\"lemma\"]][\"ru\"]\n",
    "\n",
    "for term in terms:\n",
    "    if term[\"lemma\"].endswith(\"hci\"):\n",
    "        if term[\"lemma\"].endswith(\"ahci\"):\n",
    "            singular = term[\"lemma\"][:-3]\n",
    "        else:\n",
    "            singular = term[\"lemma\"][:-2]\n",
    "\n",
    "        if singular in terms_index and not term[\"ja\"]:\n",
    "            term[\"ja\"] = next(t for t in terms if t[\"lemma\"] == singular)[\"ja\"]\n",
    "            term[\"en\"] = next(t for t in terms if t[\"lemma\"] == singular)[\"en\"]\n",
    "            term[\"ru\"] = next(t for t in terms if t[\"lemma\"] == singular)[\"ru\"]\n",
    "\n",
    "        if singular in terms_index and not term[\"poses\"]:\n",
    "            term[\"poses\"] = next(t for t in terms if t[\"lemma\"] == singular)[\"poses\"]\n",
    "\n",
    "    if term[\"lemma\"] == \"hcihi\":\n",
    "        if term[\"lemma\"].endswith(\"ahcihi\"):\n",
    "            singular = term[\"lemma\"][:-4]\n",
    "        else:\n",
    "            singular = term[\"lemma\"][:-3]  \n",
    "        \n",
    "        if not term[\"poses\"]:\n",
    "            term[\"poses\"] = [\"n\"]\n",
    "\n",
    "        if singular in terms_index and not term[\"ja\"]:\n",
    "            term[\"ja\"] = [g + \"こと\" for g in next(t for t in terms if t[\"lemma\"] == singular)[\"ja\"] if g.startswith(\"n\") ]\n",
    "            term[\"en\"] = [\"that \" + g for g in next(t for t in terms if t[\"lemma\"] == singular)[\"en\"] if g.startswith(\"n\") ]\n",
    "            term[\"ru\"] = [\"то \" + g for g in next(t for t in terms if t[\"lemma\"] == singular)[\"ru\"] if g.startswith(\"n\") ]\n",
    "\n",
    "POS_TABLE = {\n",
    "    \"自動詞\": \"vt\",\n",
    "    \"他動詞\": \"vi\",\n",
    "    \"複他動詞\": \"vd\",\n",
    "    \"完全動詞\": \"vc\",\n",
    "    \"名詞\": \"n\",\n",
    "    \"連体詞\": \"adn\",\n",
    "    \"形容詞\": \"adj\",\n",
    "    \"副詞\": \"adv\",\n",
    "    \"接続詞\": \"cconj\",\n",
    "    \"助詞\": \"post\",\n",
    "    \"助動詞\": \"auxv\",\n",
    "    \"auxverb\": \"auxv\",\n",
    "    \"終助詞\": \"sfp\",\n",
    "    \"接尾辞\": \"sfx\",\n",
    "    \"接頭辞\": \"pfx\",\n",
    "    \"間投詞\": \"intj\",\n",
    "    \"interj\": \"intj\",\n",
    "    \"後置副詞\": \"padv\",\n",
    "    \"人称接辞\": \"pers\",\n",
    "    \"繋辞\": \"cop\",\n",
    "    \"位置名詞\": \"nl\",\n",
    "    \"複数形\": \"pl\",\n",
    "    \"副助詞\": \"advp\",\n",
    "    \"suffix\": \"sfx\",\n",
    "    \"prefix\": \"pfx\",\n",
    "    \"verb\": \"v\", # TODO: Get verb slot from Wiktionary\n",
    "    \"代名詞\": \"pron\",\n",
    "    \"格助詞\": \"postp\",\n",
    "    \"接続助詞\": \"sconj\",\n",
    "    \"疑問詞\": \"int\",\n",
    "    \"形式名詞\": \"nmlz\",\n",
    "    \"noun\": \"n\"\n",
    "}\n",
    "\n",
    "\n",
    "for term in terms:\n",
    "    term[\"poses\"] = [POS_TABLE.get(p, None) or p for p in term[\"poses\"]]\n",
    "\n",
    "all_poses = set()\n",
    "for term in terms:\n",
    "    for p in term[\"poses\"]:\n",
    "        all_poses.add(p)\n",
    "for p in sorted(all_poses):\n",
    "    print(p)\n",
    "\n",
    "terms = [t for t in terms if t.get(\"frequency\", 0) > 1 or t[\"ja\"]]\n",
    "\n",
    "terms = [t for t in terms if not regex.match(r\"^\\d+$\", t[\"lemma\"])]\n",
    "\n",
    "\n",
    "# for lemma, glosses in translated.items():\n",
    "#     if lemma in terms_index:\n",
    "#         terms[terms_index.index(lemma)][\"en\"] = glosses[\"en\"]\n",
    "#         terms[terms_index.index(lemma)][\"ru\"] = glosses[\"ru\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"input/sakhalin/additional_terms.json\") as f:\n",
    "    additional_terms = json.load(f)\n",
    "    terms.extend(additional_terms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"output/ff-ainu-karahuto-terms-with-corpus.json\", \"w\") as f:\n",
    "    json.dump(terms, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI Translations from Japanese to English and Russian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All terms: 2832\n",
      "All translated: 1188\n",
      "Translated only ja: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"All terms:\", len(terms))\n",
    "\n",
    "all_translated = [t for t in terms if t[\"ja\"]]\n",
    "print(\"All translated:\", len(all_translated))\n",
    "\n",
    "translated_only_ja = [t for t in all_translated if \"en\" not in t or not t[\"en\"]]\n",
    "print(\"Translated only ja:\", len(translated_only_ja))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# Generate templates for AI to fill in\n",
    "\n",
    "import json\n",
    "\n",
    "untranslated = []\n",
    "\n",
    "split_terms = [\n",
    "    translated_only_ja[i : i + 250] for i in range(0, len(translated_only_ja), 250)\n",
    "]\n",
    "\n",
    "print(len(split_terms))\n",
    "\n",
    "UNTRANSLATED_DIR = Path(\"output/karahuto-untranslated\")\n",
    "UNTRANSLATED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for file in UNTRANSLATED_DIR.glob(\"*.json\"):\n",
    "    file.unlink()\n",
    "\n",
    "for i, t in enumerate(split_terms):\n",
    "    print(len(t))\n",
    "    with open(\n",
    "        UNTRANSLATED_DIR / f\"ff-ainu-karahuto-terms-with-corpus-untranslated-{i}.json\",\n",
    "        \"w\",\n",
    "    ) as f:\n",
    "        json.dump(\n",
    "            {\n",
    "                term[\"lemma\"]: {\"ja\": term[\"ja\"], \"en\": [], \"ru\": []}\n",
    "                for term in t\n",
    "                if term[\"ja\"]\n",
    "            },\n",
    "            f,\n",
    "            ensure_ascii=False,\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
