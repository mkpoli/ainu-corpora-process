{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Karahuto data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and clean ff-ainu data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import TypedDict, cast\n",
    "\n",
    "class RawTerm(TypedDict):\n",
    "    lemma: str\n",
    "    glosses: list[str]\n",
    "    poses: list[str]\n",
    "\n",
    "with open('output/ff-ainu-karahuto-terms.json') as f:\n",
    "    raw_terms = cast(list[RawTerm], json.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lemma': 'asne', 'ja': ['５つの'], 'en': [], 'ru': [], 'poses': ['連体詞'], 'frequency': 0, 'cognates': [], 'noncognates': []}\n",
      "{'lemma': 'asneh', 'ja': ['５つ'], 'en': [], 'ru': [], 'poses': ['名詞'], 'frequency': 0, 'cognates': [], 'noncognates': []}\n",
      "{'lemma': 'ahkas', 'ja': ['歩く'], 'en': [], 'ru': [], 'poses': ['自動詞'], 'frequency': 0, 'cognates': [], 'noncognates': []}\n",
      "{'lemma': 'ahkapo', 'ja': ['弟', '坊や'], 'en': [], 'ru': [], 'poses': ['名詞'], 'frequency': 0, 'cognates': [], 'noncognates': []}\n",
      "{'lemma': 'ahkapoho', 'ja': ['弟', '坊や'], 'en': [], 'ru': [], 'poses': ['名詞'], 'frequency': 0, 'cognates': [], 'noncognates': []}\n"
     ]
    }
   ],
   "source": [
    "class Term(TypedDict):\n",
    "    lemma: str\n",
    "    ja: list[str]\n",
    "    en: list[str]\n",
    "    ru: list[str]\n",
    "    poses: list[str]\n",
    "    frequency: int\n",
    "    cognates: list[str]\n",
    "    noncognates: list[str]\n",
    "\n",
    "terms: list[Term] = [\n",
    "    {\n",
    "        \"lemma\": term[\"lemma\"],\n",
    "        \"ja\": term[\"glosses\"],\n",
    "        \"en\": [],\n",
    "        \"ru\": [],\n",
    "        \"poses\": term[\"poses\"],\n",
    "        \"frequency\": 0,\n",
    "        \"cognates\": [],\n",
    "        \"noncognates\": [],\n",
    "    }\n",
    "    for term in raw_terms\n",
    "]\n",
    "\n",
    "for t in terms[0:5]:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique lemmas: 1076\n"
     ]
    }
   ],
   "source": [
    "terms_index = set(term[\"lemma\"] for term in terms)\n",
    "print(\"Unique lemmas:\", len(terms_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get morphological data from Wiktionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "extrapolated_cognates = defaultdict(list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part of speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.sakhalin import extrapolate_sakhalin_from_hokkaido\n",
    "\n",
    "with open(\"output/wiktionary_ainu_part_of_speech.json\") as f:\n",
    "    part_of_speech = json.load(f)\n",
    "    extrapolated_part_of_speech = {}\n",
    "    for lemma, poses in part_of_speech.items():\n",
    "        extrapolated_form = extrapolate_sakhalin_from_hokkaido(lemma)\n",
    "        if extrapolated_form not in part_of_speech:\n",
    "            extrapolated_part_of_speech[extrapolated_form] = poses\n",
    "            extrapolated_cognates[extrapolated_form].append(lemma)\n",
    "\n",
    "combined_part_of_speech = {**part_of_speech, **extrapolated_part_of_speech}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Morpheme glosses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "extrapolated_glosses = {}\n",
    "\n",
    "with open(\"output/wiktionary_ainu_glossed_morphemes.json\") as f:\n",
    "    glossed_morphemes = json.load(f)\n",
    "    for morpheme, glosses in glossed_morphemes.items():\n",
    "        extrapolated_form = extrapolate_sakhalin_from_hokkaido(morpheme)\n",
    "        if extrapolated_form not in glossed_morphemes:\n",
    "            extrapolated_glosses[extrapolated_form] = glosses\n",
    "            extrapolated_cognates[extrapolated_form].append(morpheme)\n",
    "combined_glosses = {**glossed_morphemes, **extrapolated_glosses}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extend with AI translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input/sakhalin/ff-ainu-karahuto-terms-with-corpus-translated-12.json\n",
      "input/sakhalin/ff-ainu-karahuto-terms-with-corpus-translated-2.json\n",
      "input/sakhalin/ff-ainu-karahuto-terms-with-corpus-translated-20.json\n",
      "input/sakhalin/ff-ainu-karahuto-terms-with-corpus-translated-8.json\n",
      "input/sakhalin/ff-ainu-karahuto-terms-with-corpus-translated-15.json\n",
      "input/sakhalin/ff-ainu-karahuto-terms-with-corpus-translated-4.json\n",
      "input/sakhalin/ff-ainu-karahuto-terms-with-corpus-translated-1.json\n",
      "input/sakhalin/ff-ainu-karahuto-terms-with-corpus-translated-13.json\n",
      "input/sakhalin/ff-ainu-karahuto-terms-with-corpus-translated-16.json\n",
      "input/sakhalin/ff-ainu-karahuto-terms-with-corpus-translated-10.json\n",
      "input/sakhalin/ff-ainu-karahuto-terms-with-corpus-translated-19.json\n",
      "input/sakhalin/ff-ainu-karahuto-terms-with-corpus-translated-7.json\n",
      "input/sakhalin/ff-ainu-karahuto-terms-with-corpus-translated-14.json\n",
      "input/sakhalin/ff-ainu-karahuto-terms-with-corpus-translated-17.json\n",
      "input/sakhalin/ff-ainu-karahuto-terms-with-corpus-translated-0.json\n",
      "input/sakhalin/ff-ainu-karahuto-terms-with-corpus-translated-9.json\n",
      "input/sakhalin/ff-ainu-karahuto-terms-with-corpus-translated-3.json\n",
      "input/sakhalin/ff-ainu-karahuto-terms-with-corpus-translated-11.json\n",
      "input/sakhalin/ff-ainu-karahuto-terms-with-corpus-translated-18.json\n",
      "input/sakhalin/ff-ainu-karahuto-terms-with-corpus-translated-5.json\n",
      "input/sakhalin/ff-ainu-karahuto-terms-with-corpus-translated-6.json\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "for file in (Path(\"input\") / \"sakhalin\").glob(\n",
    "    \"ff-ainu-karahuto-terms-with-corpus-translated-*.json\"\n",
    "):\n",
    "    print(file)\n",
    "    with open(file) as f:\n",
    "        translated = json.load(f)\n",
    "\n",
    "        for term in terms:\n",
    "            if term[\"lemma\"] in translated:\n",
    "                term[\"ja\"] = translated[term[\"lemma\"]][\"ja\"]\n",
    "                term[\"en\"] = translated[term[\"lemma\"]][\"en\"]\n",
    "                term[\"ru\"] = translated[term[\"lemma\"]][\"ru\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add words from corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4403\n",
      "[('taa', 6713), ('manu', 2388), ('teh', 1353), ('ike', 1208), ('kusu', 1197), ('nah', 1092), ('tani', 1072), ('neanpe', 934), ('orowa', 898), ('horokewpo', 897)]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import regex\n",
    "\n",
    "SAKHALIN_BOOKS = {\n",
    "    \"からふとのアイヌご（入門）\",\n",
    "    \"カラフトのアイヌ語（中級）\",\n",
    "    \"カラフトのアイヌ語（初級）\",\n",
    "    \"ニューエクスプレス・スペシャル 日本語の隣人たち I+II\",\n",
    "    \"ピウスツキ記念碑\",\n",
    "    \"千徳太郎治のピウスツキ宛書簡\",\n",
    "    \"浅井タケ昔話全集I,II\",\n",
    "}\n",
    "\n",
    "words: dict[str, int] = defaultdict(int)\n",
    "\n",
    "for book in SAKHALIN_BOOKS:\n",
    "    with open(\"../corpus/output/words_by_book/\" + book + \".tsv\") as f:\n",
    "        for line in f:\n",
    "            word, freq = line.strip().split(\"\\t\")\n",
    "\n",
    "            if \"mp\" in word:\n",
    "                word = word.replace(\"mp\", \"np\")\n",
    "\n",
    "            if word in [\"m\", \"horo\", \"hetunaa\", \"pii\", \"cooruntee\", \"okta\"]:\n",
    "                continue\n",
    "\n",
    "            if (len(word) > 3 and word.endswith(\"=\")) or word.endswith(\"?\"):\n",
    "                word = word[:-1]\n",
    "            if not word:\n",
    "                continue\n",
    "\n",
    "            if regex.match(r\"^[\\d\\p{P}]+$\", word):\n",
    "                continue\n",
    "\n",
    "            words[word] += int(freq)\n",
    "\n",
    "print(len(words))\n",
    "\n",
    "print(sorted(words.items(), key=lambda x: x[1], reverse=True)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, freq in words.items():\n",
    "    if word not in terms_index:\n",
    "        terms.append(\n",
    "            {\n",
    "                \"lemma\": word,\n",
    "                \"ja\": combined_glosses.get(word, []),\n",
    "                \"en\": [],\n",
    "                \"ru\": [],\n",
    "                \"poses\": combined_part_of_speech.get(word, []),\n",
    "                \"frequency\": freq,\n",
    "                \"cognates\": extrapolated_cognates.get(word, []),\n",
    "                \"noncognates\": [],\n",
    "            }\n",
    "        )\n",
    "    else:\n",
    "        term = next(term for term in terms if term[\"lemma\"] == word)\n",
    "        term[\"frequency\"] = freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect cognates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opista ['opitta', 'opitta']\n",
      "uepekere ['uepeker']\n",
      "ruru ['rur', 'rur']\n",
      "oara ['oar']\n",
      "ruh ['rup']\n",
      "ton ['tom']\n",
      "ah ['ak', 'at', 'ak', 'at']\n",
      "kikiri ['kikir']\n",
      "kan ['kam']\n",
      "sahte ['sapte']\n",
      "suh ['sut']\n",
      "kah ['kat', 'kap', 'kap', 'kat']\n",
      "tarah ['tarap']\n",
      "uhsoro ['upsor', 'upsoro']\n",
      "moro ['mor']\n",
      "henpara ['hempara']\n",
      "tun ['tum']\n",
      "henpah ['hempak']\n",
      "mahkarakuhu ['matkarkuhu']\n",
      "etaraka ['etarka']\n",
      "punkara ['punkar']\n",
      "ekah ['ekap']\n",
      "uhsoroho ['upsoroho']\n",
      "horokew ['horkew']\n",
      "ahtus ['attus']\n",
      "senpirike ['sempirke']\n",
      "epirika ['epirka']\n",
      "yaykara ['yaykar']\n",
      "niteh ['nitek']\n",
      "santeh ['santek']\n",
      "kosmah ['kosmat']\n",
      "unukara ['unukar']\n"
     ]
    }
   ],
   "source": [
    "for term in terms:\n",
    "    if term.get(\"cognates\", None):\n",
    "        print(term[\"lemma\"], term[\"cognates\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extend by morphology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "for term in terms:\n",
    "    if term[\"lemma\"].endswith(\"hci\"):\n",
    "        if term[\"lemma\"].endswith(\"ahci\"):\n",
    "            singular = term[\"lemma\"][:-3]\n",
    "        else:\n",
    "            singular = term[\"lemma\"][:-2]\n",
    "\n",
    "        if singular in terms_index and not term[\"ja\"]:\n",
    "            term[\"ja\"] = next(t for t in terms if t[\"lemma\"] == singular)[\"ja\"]\n",
    "            term[\"en\"] = next(t for t in terms if t[\"lemma\"] == singular)[\"en\"]\n",
    "            term[\"ru\"] = next(t for t in terms if t[\"lemma\"] == singular)[\"ru\"]\n",
    "\n",
    "        if singular in terms_index and not term[\"poses\"]:\n",
    "            term[\"poses\"] = next(t for t in terms if t[\"lemma\"] == singular)[\"poses\"]\n",
    "\n",
    "    if term[\"lemma\"] == \"hcihi\":\n",
    "        if term[\"lemma\"].endswith(\"ahcihi\"):\n",
    "            singular = term[\"lemma\"][:-4]\n",
    "        else:\n",
    "            singular = term[\"lemma\"][:-3]\n",
    "\n",
    "        if not term[\"poses\"]:\n",
    "            term[\"poses\"] = [\"n\"]\n",
    "\n",
    "        if singular in terms_index and not term[\"ja\"]:\n",
    "            term[\"ja\"] = [\n",
    "                g + \"こと\"\n",
    "                for g in next(t for t in terms if t[\"lemma\"] == singular)[\"ja\"]\n",
    "                if g.startswith(\"n\")\n",
    "            ]\n",
    "            term[\"en\"] = [\n",
    "                \"that \" + g\n",
    "                for g in next(t for t in terms if t[\"lemma\"] == singular)[\"en\"]\n",
    "                if g.startswith(\"n\")\n",
    "            ]\n",
    "            term[\"ru\"] = [\n",
    "                \"то \" + g\n",
    "                for g in next(t for t in terms if t[\"lemma\"] == singular)[\"ru\"]\n",
    "                if g.startswith(\"n\")\n",
    "            ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extend POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adn\n",
      "adv\n",
      "advp\n",
      "auxv\n",
      "cconj\n",
      "colloc\n",
      "cop\n",
      "int\n",
      "intj\n",
      "n\n",
      "nl\n",
      "nmlz\n",
      "num\n",
      "padv\n",
      "parti\n",
      "pers\n",
      "pfx\n",
      "pl\n",
      "postp\n",
      "pron\n",
      "rel\n",
      "root\n",
      "sconj\n",
      "sfp\n",
      "sfx\n",
      "v\n",
      "vc\n",
      "vd\n",
      "vi\n",
      "vt\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import regex\n",
    "from typing import TypedDict\n",
    "\n",
    "class Word(TypedDict):\n",
    "    lemma: str\n",
    "    glosses: list[str]\n",
    "    poses: list[str]\n",
    "    notes: str\n",
    "    frequency: int\n",
    "\n",
    "POS_TABLE = {\n",
    "    \"自動詞\": \"vt\",\n",
    "    \"他動詞\": \"vi\",\n",
    "    \"複他動詞\": \"vd\",\n",
    "    \"完全動詞\": \"vc\",\n",
    "    \"名詞\": \"n\",\n",
    "    \"連体詞\": \"adn\",\n",
    "    \"形容詞\": \"adj\",\n",
    "    \"副詞\": \"adv\",\n",
    "    \"接続詞\": \"cconj\",\n",
    "    \"助詞\": \"post\",\n",
    "    \"助動詞\": \"auxv\",\n",
    "    \"auxverb\": \"auxv\",\n",
    "    \"終助詞\": \"sfp\",\n",
    "    \"接尾辞\": \"sfx\",\n",
    "    \"接頭辞\": \"pfx\",\n",
    "    \"間投詞\": \"intj\",\n",
    "    \"interj\": \"intj\",\n",
    "    \"後置副詞\": \"padv\",\n",
    "    \"人称接辞\": \"pers\",\n",
    "    \"繋辞\": \"cop\",\n",
    "    \"位置名詞\": \"nl\",\n",
    "    \"複数形\": \"pl\",\n",
    "    \"副助詞\": \"advp\",\n",
    "    \"suffix\": \"sfx\",\n",
    "    \"prefix\": \"pfx\",\n",
    "    \"verb\": \"v\", # TODO: Get verb slot from Wiktionary\n",
    "    \"代名詞\": \"pron\",\n",
    "    \"格助詞\": \"postp\",\n",
    "    \"接続助詞\": \"sconj\",\n",
    "    \"疑問詞\": \"int\",\n",
    "    \"形式名詞\": \"nmlz\",\n",
    "    \"noun\": \"n\"\n",
    "}\n",
    "\n",
    "\n",
    "for term in terms:\n",
    "    term[\"poses\"] = [POS_TABLE.get(p, None) or p for p in term[\"poses\"]]\n",
    "\n",
    "all_poses = set()\n",
    "for term in terms:\n",
    "    for p in term[\"poses\"]:\n",
    "        all_poses.add(p)\n",
    "for p in sorted(all_poses):\n",
    "    print(p)\n",
    "\n",
    "terms = [t for t in terms if t.get(\"frequency\", 0) > 1 or t[\"ja\"]]\n",
    "\n",
    "terms = [t for t in terms if not regex.match(r\"^\\d+$\", t[\"lemma\"])]\n",
    "\n",
    "\n",
    "# for lemma, glosses in translated.items():\n",
    "#     if lemma in terms_index:\n",
    "#         terms[terms_index.index(lemma)][\"en\"] = glosses[\"en\"]\n",
    "#         terms[terms_index.index(lemma)][\"ru\"] = glosses[\"ru\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add manually added terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"input/sakhalin/additional_terms.json\") as f:\n",
    "    additional_terms = cast(list[Term], json.load(f))\n",
    "\n",
    "    for term in additional_terms:\n",
    "        t = next((t for t in terms if t[\"lemma\"] == term[\"lemma\"]), None)\n",
    "        if t is None:\n",
    "            terms.append(term)\n",
    "        else:\n",
    "            t[\"ja\"] = t[\"ja\"] + term[\"ja\"]\n",
    "            t[\"en\"] = t.get(\"en\", []) + term[\"en\"]\n",
    "            t[\"ru\"] = t.get(\"ru\", []) + term[\"ru\"]\n",
    "            t[\"frequency\"] = t.get(\"frequency\", 0) + term.get(\"frequency\", 0)\n",
    "            t[\"poses\"] = t.get(\"poses\", []) + term.get(\"poses\", [])\n",
    "            t[\"cognates\"] = t.get(\"cognates\", []) + term.get(\"cognates\", [])\n",
    "            t[\"noncognates\"] = t.get(\"noncognates\", []) + term.get(\"noncognates\", [])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lemma': 'manuy', 'ja': ['という', 'そうだ'], 'en': ['called', 'it seems'], 'ru': ['называется', 'кажется'], 'poses': ['auxv'], 'frequency': 2388, 'cognates': [], 'noncognates': []}\n"
     ]
    }
   ],
   "source": [
    "from typing import TypedDict, cast, NotRequired\n",
    "\n",
    "PartialTerm = TypedDict(\"PartialTerm\", {\"ja\": list[str], \"en\": list[str], \"ru\": list[str], \"poses\": list[str], \"frequency\": int}, total=False)\n",
    "\n",
    "DerivedFrom = TypedDict(\"DerivedFrom\", {\"lemma\": str, \"from\": str, \"overwrite\": NotRequired[PartialTerm]  })\n",
    "\n",
    "with open(\"input/sakhalin/derived_from.json\") as f:\n",
    "    derived_from = cast(list[DerivedFrom], json.load(f))\n",
    "\n",
    "    for term in derived_from:\n",
    "        if term[\"from\"] not in terms_index:\n",
    "            print(f\"{term['from']} not found\")\n",
    "            continue\n",
    "\n",
    "        found_term = next(t for t in terms if t[\"lemma\"] == term[\"from\"])\n",
    "        overwrite: PartialTerm = term.get(\"overwrite\", {})\n",
    "\n",
    "        constructed_term: Term = {\n",
    "            \"lemma\": term[\"lemma\"],\n",
    "            \"ja\": overwrite.get(\"ja\", []) or found_term[\"ja\"],\n",
    "            \"en\": term.get(\"overwrite\", {}).get(\"en\", []) or found_term[\"en\"],\n",
    "            \"ru\": term.get(\"overwrite\", {}).get(\"ru\", []) or found_term[\"ru\"],\n",
    "            \"poses\": term.get(\"overwrite\", {}).get(\"poses\", []) or found_term[\"poses\"],\n",
    "            \"frequency\": term.get(\"overwrite\", {}).get(\"frequency\", 0)\n",
    "            or found_term[\"frequency\"],\n",
    "            \"cognates\": term.get(\"overwrite\", {}).get(\"cognates\", []) or found_term[\"cognates\"],\n",
    "            \"noncognates\": term.get(\"overwrite\", {}).get(\"noncognates\", []) or found_term[\"noncognates\"],\n",
    "        }\n",
    "\n",
    "        print(constructed_term)\n",
    "\n",
    "        target_term = next((t for t in terms if t[\"lemma\"] == term[\"lemma\"]), None)\n",
    "\n",
    "        if target_term is None:\n",
    "            terms.append(constructed_term)\n",
    "        else:\n",
    "            target_term[\"ja\"] = constructed_term[\"ja\"]\n",
    "            target_term[\"en\"] = constructed_term[\"en\"]\n",
    "            target_term[\"ru\"] = constructed_term[\"ru\"]\n",
    "            target_term[\"poses\"] = constructed_term[\"poses\"]\n",
    "            target_term[\"frequency\"] = constructed_term[\"frequency\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# override\n",
    "\n",
    "with open(\"input/sakhalin/additional_override.json\") as f:\n",
    "    additional_override = cast(list[Term], json.load(f))\n",
    "\n",
    "    for term in additional_override:\n",
    "        t = next((t for t in terms if t[\"lemma\"] == term[\"lemma\"] and t[\"ja\"] == term[\"ja\"]), None)\n",
    "        if t is None:\n",
    "            print(f\"{term['lemma']} not found\")\n",
    "            continue\n",
    "\n",
    "        for k, v in term.get(\"overwrite\", {}).items():\n",
    "            t[k] = v\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the JSON slimmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = [{\n",
    "    k: v\n",
    "    for k, v in t.items()\n",
    "    if v != []\n",
    "} for t in terms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"output/ff-ainu-karahuto-terms-with-corpus.json\", \"w\") as f:\n",
    "    json.dump(terms, f, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'ja'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[127], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(terms, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0\u001b[39m), reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mt\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mja\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m:\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;28mprint\u001b[39m(json\u001b[38;5;241m.\u001b[39mdumps({\n\u001b[1;32m      5\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlemma\u001b[39m\u001b[38;5;124m\"\u001b[39m: t[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlemma\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      6\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mja\u001b[39m\u001b[38;5;124m\"\u001b[39m: [],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     11\u001b[0m         }, ensure_ascii\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'ja'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "for t in sorted(terms, key=lambda x: x.get(\"frequency\", 0), reverse=True):\n",
    "    if not t[\"ja\"]:\n",
    "        print(json.dumps({\n",
    "            \"lemma\": t[\"lemma\"],\n",
    "            \"ja\": [],\n",
    "            \"en\": [],\n",
    "            \"ru\": [],\n",
    "            \"poses\": t[\"poses\"],\n",
    "            \"frequency\": 0\n",
    "        }, ensure_ascii=False) + \",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI Translations from Japanese to English and Russian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All terms: 2782\n",
      "All translated: 1236\n",
      "Translated only ja: 48\n"
     ]
    }
   ],
   "source": [
    "print(\"All terms:\", len(terms))\n",
    "\n",
    "all_translated = [t for t in terms if t[\"ja\"]]\n",
    "print(\"All translated:\", len(all_translated))\n",
    "\n",
    "translated_only_ja = [t for t in all_translated if \"en\" not in t or not t[\"en\"]]\n",
    "print(\"Translated only ja:\", len(translated_only_ja))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "48\n"
     ]
    }
   ],
   "source": [
    "# Generate templates for AI to fill in\n",
    "\n",
    "import json\n",
    "\n",
    "untranslated = []\n",
    "\n",
    "split_terms = [\n",
    "    translated_only_ja[i : i + 250] for i in range(0, len(translated_only_ja), 250)\n",
    "]\n",
    "\n",
    "print(len(split_terms))\n",
    "\n",
    "UNTRANSLATED_DIR = Path(\"output/karahuto-untranslated\")\n",
    "UNTRANSLATED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for file in UNTRANSLATED_DIR.glob(\"*.json\"):\n",
    "    file.unlink()\n",
    "\n",
    "for i, t in enumerate(split_terms):\n",
    "    print(len(t))\n",
    "    with open(\n",
    "        UNTRANSLATED_DIR / f\"ff-ainu-karahuto-terms-with-corpus-untranslated-{i}.json\",\n",
    "        \"w\",\n",
    "    ) as f:\n",
    "        json.dump(\n",
    "            {\n",
    "                term[\"lemma\"]: {\"ja\": term[\"ja\"], \"en\": [], \"ru\": []}\n",
    "                for term in t\n",
    "                if term[\"ja\"]\n",
    "            },\n",
    "            f,\n",
    "            ensure_ascii=False,\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
